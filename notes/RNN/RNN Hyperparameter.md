# **Hyperparamter** 

## **MiniBatch Size **

It hs an effect on the resource requirement of the training process, but also impacts training speed and number of iteration.

### Terminology

Historically, there has been a debate whether it's better to do **online**, also called stochastic training, where you fit a single example of the dataset to the model during training step and using only one example, do a forward pass, calculate the error and then back propgate, and set adjusted values for all your parameters. And then do this agian for each example in the dataset. 

Or if it was better to feed the entire dataset to the training step and calculate the gradient using the error generated by looking at all the examples in the dataset. This is called **Batch Training**. 

### Practice

The abstraction commonly used today is to set a minibatch size. So online training is when the minibatch size is one, and batch training is when minibatch size is the same as the number of examples in the training set. We can set the minibatch size to any value between these two values. **The recommended starting values for your experimentation are between one and a few hundred with 32. (ex : [1, 2, 4, 8, 16, 32, 64, 128, 256]). 32 often being a good candidate. **

A larger minibatch size allows computational boosts that utilize the matrix multiplication in the training calculation. But that comes at the xpense of needing more memory for the training process. Some out of memory errors in TensorFlow can be eliminated by decreasing the minibatch size. It is important to note that this computational boost comes at a cost. In practice, small minibatch sizes have more noise in the error calculations, and this noise is often helpful in preventing the training process from stopping at local minima on the error curve rather than the global minima creates the best model. So while the computational boost incentivizes us to increase the minibatch size. This practical algorithmic benefit incentivizes us to actually make it smaller. So, in addition to 32, you might also want to try to experiment with 64, 128, and 256. **Therefore, a good and appropriate batch size will be 32, 64, 128, and 256. **

### Resouce

* [Systematic evaluation of CNN advances on the ImageNet](https://arxiv.org/abs/1606.02228)

## **Number of Training Iterations / Epoch**

To choose the right number of iterations or number of epochs for our training step, the metric we should have our eyes on is the validation error. The intuitive manual way is to have the model train for as many epochs or iterations that it takes, as long as the validation error keeps decreasing. Luckly, however, we can use a technique called early stopping to determien when to stop training a model. 

Early stopping roughly works by monitoring the validation error, and stopping the training when it stops decreasing. We must be a little flexible though in defining the stopping trigger. Validation error will often move back and forth, even if it's one a downward trend. So instead of stopping the training, the first time we see the validation error increase, we can instead stop the trianing if the validation error has not improved in the last 10 or 20 steps. 

The number of training iterations is a hyperparameter we can optimize automatically using a technique called early stopping (also "early termination").

## Number of Hidden Units / Layers

Hyperparameter that relate to the model itself rather than the training or optimization process. The main requirement is to set a number of hidden units  "Large Enough". For a neural network to learn to approximate a function or a prediction task, it needs to have enough "capacity to learn the function". The more complex the function, the more learning capacity the model will need. The number and architecture of the hidden units is the main measure for a model's learning capacity. If we provide the model with too much capacity, however, it might try to overfit or to memorize the training set. If you find your model overfitting your data meaning that the training accuracy is much better than the validation accuracy, you might want to decrease the number of hidden units.  You could also utilize the regularization techniques like dropout or L2 Legularization.  

"In practice, it is often the case taht 3-layer neural networks will outperform 2-layer nets, but going even deeper (4, 5, 6 layer ) rarely helps much more. This is in stark contrast to Convolutional Networks, where depth has been found to be extremly important component for a good recognition system (e.g. on order of 10 learnable layers). ~ Andrej Karpathy" [Lecture](https://cs231n.github.io/neural-networks-1/)

## **RNN Hyperparameter**

Two main choise when we want to build a recurrent neural network are choosing a cell type so a long short term memory or a vailla RNN cell or a gated recurrent unit cell, and how deep the model is, how many layers will we stack? 

### Practice

In practice, LSTMs and GRUs perform better than vanilla RNN. that much is clear. **LSTMs are seemed to be more commonly used, it really depends on the task and the dataset**.

### **LSTM vs GRU**

"These results clearly indicate the advantages of the gatign units over the more traiditional recurrent units. Convergence is often faster, and the final solutions tend to be better. However, our results are not conclusive in comparing the LSTM and the GRU, whcich suggests that the choise of the type of gated recurrent unit may depend havily on the dataset and corresponding tasks."

[Emprical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](https://arxiv.org/abs/1412.3555) says that "The GRU outperformed the LSTM on a ll tasks with the exception of language modeling". [An Empirical Exploration of Recurrent Netwrok Architectures](http://proceedings.mlr.press/v37/jozefowicz15.pdf) says that "Our consistent fidning is that depth of at least two is beneficial. However, between two and three layers our results are mixed. Additionally, the results are mixed between the LSTM and the GRU, but both significantly outperform the RNN"

"[Visualizing and Understanding Recurrent Networks](https://arxiv.org/abs/1506.02078)" is try to use both and compare. 

Regarding the number of layers, results for character level language modeling show that a depth of at least two is shown to be beneficial. But increasing it to three actually gives mixed results. Another task like advanced speech recognition can show imporvements with five and seven layers often without LSTM cells. 

## Example RNN Architectures

|              Application              | Cell | Layers  |     Size      |        Vocabulary         | Embedding Size | Learning Rate |                                                              |
| :-----------------------------------: | :--: | :-----: | :-----------: | :-----------------------: | :------------: | :-----------: | :----------------------------------------------------------: |
| Speech Recognition (large vocabulary) | LSTM |  5, 7   |   600, 1000   |         82K, 500K         |       --       |      --       |          [paper](https://arxiv.org/abs/1610.09975)           |
|          Speech Recognition           | LSTM | 1, 3, 5 |      250      |            --             |       --       |     0.001     |           [paper](https://arxiv.org/abs/1303.5778)           |
|     Machine Translation (seq2seq)     | LSTM |    4    |     1000      | Source: 160K, Target: 80K |     1,000      |      --       |           [paper](https://arxiv.org/abs/1409.3215)           |
|           Image Captioning            | LSTM |   --    |      512      |            --             |      512       |    (fixed)    |           [paper](https://arxiv.org/abs/1411.4555)           |
|           Image Generation            | LSTM |   --    | 256, 400, 800 |            --             |       --       |      --       |          [paper](https://arxiv.org/abs/1502.04623)           |
|          Question Answering           | LSTM |    2    |      500      |            --             |      300       |      --       |       [pdf](http://www.aclweb.org/anthology/P15-2116)        |
|          Text Summarization           | GRU  |         |      200      | Source: 119K, Target: 68K |      100       |     0.001     | [pdf](https://pdfs.semanticscholar.org/3fbc/45152f20403266b02c4c2adab26fb367522d.pdf) |



